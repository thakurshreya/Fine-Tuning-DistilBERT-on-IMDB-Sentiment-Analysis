{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HMYscS_tWeZ"
      },
      "outputs": [],
      "source": [
        "# @title Setup and Installation { display-mode: \"form\" }\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers datasets scikit-learn matplotlib seaborn torch tqdm tensorboard\n",
        "!pip install --upgrade transformers\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    DistilBertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    classification_report\n",
        ")\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "from tqdm.auto import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for saving models and results\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directories\n",
        "PROJECT_DIR = '/content/imdb-sentiment'\n",
        "MODEL_DIR = f'{PROJECT_DIR}/model'\n",
        "BEST_MODEL_DIR = f'{MODEL_DIR}/best_model'\n",
        "LOGS_DIR = f'{PROJECT_DIR}/logs'\n",
        "VISUALIZATIONS_DIR = f'{PROJECT_DIR}/visualizations'\n",
        "\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(BEST_MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(LOGS_DIR, exist_ok=True)\n",
        "os.makedirs(VISUALIZATIONS_DIR, exist_ok=True)\n",
        "\n",
        "# Option to save to Google Drive\n",
        "SAVE_TO_DRIVE = True  # @param {type:\"boolean\"}\n",
        "if SAVE_TO_DRIVE:\n",
        "    DRIVE_DIR = '/content/drive/MyDrive/imdb-sentiment'\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configuration { display-mode: \"form\" }\n",
        "\n",
        "# Define configuration parameters individually for form widgets\n",
        "epochs = 2  # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "train_batch_size = 32  # @param {type:\"slider\", min:8, max:64, step:8}\n",
        "eval_batch_size = 64  # @param {type:\"slider\", min:8, max:64, step:8}\n",
        "learning_rate = 5e-5  # @param {type:\"number\"}\n",
        "weight_decay = 0.01  # @param {type:\"number\"}\n",
        "max_length = 256  # @param {type:\"slider\", min:128, max:512, step:32}\n",
        "sample_size = \"1000\"  # @param {type:\"string\"} # MODIFIED: Using a small sample size for faster training\n",
        "\n",
        "# Now build the config dictionary from the individual parameters\n",
        "config = {\n",
        "  \"model\": {\n",
        "    \"name\": \"distilbert-base-uncased\",\n",
        "    \"num_labels\": 2\n",
        "  },\n",
        "  \"training\": {\n",
        "    \"epochs\": epochs,\n",
        "    \"train_batch_size\": train_batch_size,\n",
        "    \"eval_batch_size\": eval_batch_size,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"weight_decay\": weight_decay,\n",
        "    \"max_length\": max_length,\n",
        "    \"save_steps\": 100,  # MODIFIED: Reduced from 500 for smaller dataset\n",
        "    \"eval_steps\": 100   # MODIFIED: Reduced from 500 for smaller dataset\n",
        "  },\n",
        "  \"data\": {\n",
        "    \"dataset\": \"imdb\",\n",
        "    \"train_size\": 25000,\n",
        "    \"test_size\": 25000,\n",
        "    \"sample_size\": None if sample_size == \"None\" else int(sample_size) if isinstance(sample_size, str) and sample_size.isdigit() else None\n",
        "  },\n",
        "  \"paths\": {\n",
        "    \"model_dir\": MODEL_DIR,\n",
        "    \"best_model_dir\": BEST_MODEL_DIR,\n",
        "    \"logs_dir\": LOGS_DIR,\n",
        "    \"visualizations_dir\": VISUALIZATIONS_DIR\n",
        "  }\n",
        "}\n",
        "\n",
        "# Save config to file\n",
        "with open(f'{PROJECT_DIR}/config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"Configuration saved!\")\n",
        "print(f\"Using sample size: {config['data']['sample_size']} examples\")"
      ],
      "metadata": {
        "id": "knYgFdM0tc6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data Preparation { display-mode: \"form\" }\n",
        "\n",
        "def prepare_dataset(sample_size=None):\n",
        "    \"\"\"\n",
        "    Load and tokenize the IMDB dataset for sentiment analysis.\n",
        "\n",
        "    Args:\n",
        "        sample_size: Optional number of examples to use (for faster testing)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_dataset, test_dataset) tokenized and formatted for PyTorch\n",
        "    \"\"\"\n",
        "    print(\"Loading IMDB dataset...\")\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "    print(f\"Dataset loaded with {len(dataset['train'])} training and {len(dataset['test'])} test examples\")\n",
        "\n",
        "    # Optionally use a smaller sample for faster testing\n",
        "    if sample_size is not None:\n",
        "        dataset['train'] = dataset['train'].shuffle(seed=42).select(range(min(sample_size, len(dataset['train']))))\n",
        "        dataset['test'] = dataset['test'].shuffle(seed=42).select(range(min(sample_size//5, len(dataset['test']))))\n",
        "        print(f\"Using {len(dataset['train'])} training and {len(dataset['test'])} test examples\")\n",
        "\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(config[\"model\"][\"name\"])\n",
        "    print(\"Tokenizing dataset...\")\n",
        "\n",
        "    def tokenize(example):\n",
        "        return tokenizer(\n",
        "            example[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=config[\"training\"][\"max_length\"]\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "    print(\"Dataset preparation complete\")\n",
        "    return tokenized_dataset[\"train\"], tokenized_dataset[\"test\"]\n",
        "\n",
        "# Run data preparation\n",
        "train_dataset, test_dataset = prepare_dataset(config[\"data\"][\"sample_size\"])"
      ],
      "metadata": {
        "id": "-MU-OZdntczE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utility Functions { display-mode: \"form\" }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics for the model.\n",
        "\n",
        "    Args:\n",
        "        eval_pred: Tuple of predictions and labels\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of metrics\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average=\"weighted\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall\n",
        "    }\n",
        "\n",
        "def save_training_plot(history, output_path):\n",
        "    \"\"\"\n",
        "    Generate and save a learning curve plot from training history.\n",
        "\n",
        "    Args:\n",
        "        history: Training history from trainer.state.log_history\n",
        "        output_path: Path to save the plot\n",
        "    \"\"\"\n",
        "    train_loss = []\n",
        "    eval_loss = []\n",
        "    eval_accuracy = []\n",
        "    steps = []\n",
        "\n",
        "    # Extract metrics from history\n",
        "    for entry in history:\n",
        "        if \"loss\" in entry and \"step\" in entry and \"epoch\" not in entry:\n",
        "            train_loss.append(entry[\"loss\"])\n",
        "            steps.append(entry[\"step\"])\n",
        "        elif \"eval_loss\" in entry:\n",
        "            eval_loss.append(entry[\"eval_loss\"])\n",
        "            eval_accuracy.append(entry[\"eval_accuracy\"])\n",
        "\n",
        "    # Create figure with two y-axes\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Plot training loss\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Training Steps')\n",
        "    ax1.set_ylabel('Loss', color=color)\n",
        "\n",
        "    # Check if we have training loss data\n",
        "    if train_loss and steps:\n",
        "        ax1.plot(steps, train_loss, color=color, label='Training Loss')\n",
        "\n",
        "    # Check if we have evaluation loss data\n",
        "    if eval_loss and steps:\n",
        "        try:\n",
        "            # Only plot evaluation loss if we have both training steps and eval loss\n",
        "            if len(steps) > 0:\n",
        "                # Calculate evaluation steps safely\n",
        "                last_step = steps[-1] if steps else 0\n",
        "                eval_steps = [last_step * (i+1) / len(eval_loss) for i in range(len(eval_loss))]\n",
        "                ax1.plot(eval_steps, eval_loss, 'o-', color='tab:green', label='Evaluation Loss')\n",
        "        except (IndexError, ZeroDivisionError) as e:\n",
        "            print(f\"Warning: Could not plot evaluation loss: {e}\")\n",
        "\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Create second y-axis for accuracy\n",
        "    if eval_accuracy and steps:\n",
        "        try:\n",
        "            ax2 = ax1.twinx()\n",
        "            color = 'tab:red'\n",
        "            ax2.set_ylabel('Accuracy', color=color)\n",
        "\n",
        "            # Calculate evaluation steps safely (same as above)\n",
        "            last_step = steps[-1] if steps else 0\n",
        "            eval_steps = [last_step * (i+1) / len(eval_accuracy) for i in range(len(eval_accuracy))]\n",
        "\n",
        "            ax2.plot(eval_steps, eval_accuracy, 'o-', color=color, label='Evaluation Accuracy')\n",
        "            ax2.tick_params(axis='y', labelcolor=color)\n",
        "            ax2.set_ylim([0, 1])\n",
        "\n",
        "            # Add legend\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
        "        except (IndexError, ZeroDivisionError) as e:\n",
        "            print(f\"Warning: Could not plot evaluation accuracy: {e}\")\n",
        "    else:\n",
        "        # Add legend for just training loss\n",
        "        ax1.legend(loc='best')\n",
        "\n",
        "    plt.title('Training and Evaluation Metrics')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(output_path)\n",
        "\n",
        "    # Also display the plot in the notebook\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JC_g_D_stcuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model Training { display-mode: \"form\" }\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"\n",
        "    Train a DistilBERT model on the IMDB dataset for sentiment classification.\n",
        "\n",
        "    Returns:\n",
        "        Trainer: The trained model trainer\n",
        "    \"\"\"\n",
        "    print(\"Initializing DistilBERT model...\")\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        config[\"model\"][\"name\"],\n",
        "        num_labels=config[\"model\"][\"num_labels\"]\n",
        "    )\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config[\"paths\"][\"model_dir\"],\n",
        "        eval_strategy=\"epoch\",  # FIXED: Changed from evaluation_strategy to eval_strategy\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=config[\"paths\"][\"logs_dir\"],\n",
        "        logging_steps=config[\"training\"][\"save_steps\"],\n",
        "        num_train_epochs=config[\"training\"][\"epochs\"],\n",
        "        per_device_train_batch_size=config[\"training\"][\"train_batch_size\"],\n",
        "        per_device_eval_batch_size=config[\"training\"][\"eval_batch_size\"],\n",
        "        learning_rate=config[\"training\"][\"learning_rate\"],\n",
        "        weight_decay=config[\"training\"][\"weight_decay\"],\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        report_to=\"tensorboard\"\n",
        "    )\n",
        "\n",
        "    print(\"Initializing trainer...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    # Save training metrics\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "    # Save the model\n",
        "    print(\"Saving best model...\")\n",
        "    trainer.save_model(config[\"paths\"][\"best_model_dir\"])\n",
        "\n",
        "    # Generate and save learning curve\n",
        "    print(\"Generating learning curve...\")\n",
        "    history = trainer.state.log_history\n",
        "    save_training_plot(history, os.path.join(config[\"paths\"][\"visualizations_dir\"], \"learning_curve.png\"))\n",
        "\n",
        "    # Copy to Google Drive if requested\n",
        "    if SAVE_TO_DRIVE:\n",
        "        print(\"Copying model to Google Drive...\")\n",
        "        !cp -r {config[\"paths\"][\"best_model_dir\"]} {DRIVE_DIR}/\n",
        "        !cp {os.path.join(config[\"paths\"][\"visualizations_dir\"], \"learning_curve.png\")} {DRIVE_DIR}/\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return trainer\n",
        "\n",
        "# Run training\n",
        "trainer = train_model()"
      ],
      "metadata": {
        "id": "1ui-5tZytclB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model Evaluation { display-mode: \"form\" }\n",
        "\n",
        "def evaluate_model(trainer=None):\n",
        "    \"\"\"\n",
        "    Evaluate the trained DistilBERT model on the IMDB test dataset.\n",
        "\n",
        "    Args:\n",
        "        trainer: Optional pre-initialized trainer\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation metrics\n",
        "    \"\"\"\n",
        "    if trainer is None:\n",
        "        print(\"Loading trained model...\")\n",
        "        model_path = config[\"paths\"][\"best_model_dir\"]\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model not found at {model_path}. Please train the model first.\")\n",
        "\n",
        "        model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "        print(\"Setting up trainer for evaluation...\")\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "    print(\"Evaluating model...\")\n",
        "    metrics = trainer.evaluate(test_dataset)\n",
        "\n",
        "    print(\"\\n===== Evaluation Results =====\")\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "    # Save metrics to file\n",
        "    with open(os.path.join(config[\"paths\"][\"model_dir\"], \"evaluation_results.json\"), \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    print(f\"Evaluation results saved to {os.path.join(config['paths']['model_dir'], 'evaluation_results.json')}\")\n",
        "\n",
        "    # Copy to Google Drive if requested\n",
        "    if SAVE_TO_DRIVE:\n",
        "        !cp {os.path.join(config[\"paths\"][\"model_dir\"], \"evaluation_results.json\")} {DRIVE_DIR}/\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Run evaluation\n",
        "metrics = evaluate_model(trainer)"
      ],
      "metadata": {
        "id": "F_xoCzjXtcgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Error Analysis { display-mode: \"form\" }\n",
        "\n",
        "def error_analysis(trainer=None):\n",
        "    \"\"\"\n",
        "    Perform error analysis on the trained model.\n",
        "\n",
        "    Args:\n",
        "        trainer: Optional pre-initialized trainer\n",
        "\n",
        "    Returns:\n",
        "        dict: Error analysis results\n",
        "    \"\"\"\n",
        "    # Initialize trainer if not provided\n",
        "    if trainer is None:\n",
        "        model_path = config[\"paths\"][\"best_model_dir\"]\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model not found at {model_path}. Please train the model first.\")\n",
        "\n",
        "        model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "        trainer = Trainer(model=model)\n",
        "\n",
        "    print(\"Running predictions on test dataset...\")\n",
        "    preds_output = trainer.predict(test_dataset)\n",
        "\n",
        "    # Get predictions and labels\n",
        "    preds = torch.argmax(torch.tensor(preds_output.predictions), dim=1)\n",
        "    labels = torch.tensor(preds_output.label_ids)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    print(\"Generating confusion matrix...\")\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    # Plot and save confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
        "    disp.plot(cmap=plt.cm.Blues, values_format=\"d\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.savefig(os.path.join(config[\"paths\"][\"visualizations_dir\"], \"confusion_matrix.png\"))\n",
        "    print(f\"Confusion matrix saved to {os.path.join(config['paths']['visualizations_dir'], 'confusion_matrix.png')}\")\n",
        "\n",
        "    # Display the confusion matrix\n",
        "    plt.show()\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(labels, preds, target_names=[\"Negative\", \"Positive\"], output_dict=True)\n",
        "    print(\"\\n===== Classification Report =====\")\n",
        "    print(classification_report(labels, preds, target_names=[\"Negative\", \"Positive\"]))\n",
        "\n",
        "    # Find misclassified examples\n",
        "    misclassified_indices = (preds != labels).nonzero(as_tuple=True)[0]\n",
        "\n",
        "    # Get a sample of misclassified examples\n",
        "    num_examples = min(10, len(misclassified_indices))\n",
        "    sample_indices = np.random.choice(misclassified_indices, num_examples, replace=False)\n",
        "\n",
        "    # Get original text for misclassified examples\n",
        "    original_dataset = test_dataset.dataset\n",
        "\n",
        "    print(f\"\\n===== Sample of {num_examples} Misclassified Examples =====\")\n",
        "    misclassified_examples = []\n",
        "    for idx in sample_indices:\n",
        "        true_label = \"Positive\" if labels[idx] == 1 else \"Negative\"\n",
        "        pred_label = \"Positive\" if preds[idx] == 1 else \"Negative\"\n",
        "\n",
        "        # Get the original text (this assumes the dataset has the original text)\n",
        "        original_idx = test_dataset.indices[idx] if hasattr(test_dataset, 'indices') else idx\n",
        "        text = original_dataset[original_idx]['text']\n",
        "\n",
        "        # Truncate text if too long\n",
        "        if len(text) > 100:\n",
        "            text = text[:100] + \"...\"\n",
        "\n",
        "        example = {\n",
        "            \"text\": text,\n",
        "            \"true_label\": true_label,\n",
        "            \"predicted_label\": pred_label\n",
        "        }\n",
        "        misclassified_examples.append(example)\n",
        "\n",
        "        print(f\"Example {idx}:\")\n",
        "        print(f\"Text: {text}\")\n",
        "        print(f\"True label: {true_label}\")\n",
        "        print(f\"Predicted label: {pred_label}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Copy to Google Drive if requested\n",
        "    if SAVE_TO_DRIVE:\n",
        "        !cp {os.path.join(config[\"paths\"][\"visualizations_dir\"], \"confusion_matrix.png\")} {DRIVE_DIR}/\n",
        "\n",
        "    # Return analysis results\n",
        "    results = {\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"classification_report\": report,\n",
        "        \"misclassified_examples\": misclassified_examples\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run error analysis\n",
        "analysis_results = error_analysis(trainer)"
      ],
      "metadata": {
        "id": "mM9Lnq-mtcam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Inference { display-mode: \"form\" }\n",
        "\n",
        "def load_sentiment_pipeline():\n",
        "    \"\"\"\n",
        "    Load the sentiment analysis pipeline with the trained model.\n",
        "\n",
        "    Returns:\n",
        "        pipeline: Hugging Face pipeline for sentiment analysis\n",
        "    \"\"\"\n",
        "    from transformers import pipeline\n",
        "\n",
        "    model_path = config[\"paths\"][\"best_model_dir\"]\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model not found at {model_path}. Please train the model first.\")\n",
        "\n",
        "    print(f\"Loading sentiment analysis pipeline from {model_path}...\")\n",
        "    sentiment_pipeline = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=model_path,\n",
        "        tokenizer=\"distilbert-base-uncased\",\n",
        "        device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        "    )\n",
        "    return sentiment_pipeline\n",
        "\n",
        "def predict(text, pipeline=None):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to analyze\n",
        "        pipeline: Optional pre-loaded pipeline\n",
        "\n",
        "    Returns:\n",
        "        dict: Prediction result with label and score\n",
        "    \"\"\"\n",
        "    if pipeline is None:\n",
        "        pipeline = load_sentiment_pipeline()\n",
        "\n",
        "    start_time = time.time()\n",
        "    result = pipeline(text)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Map label index to sentiment\n",
        "    label_map = {0: \"Negative\", 1: \"Positive\"}\n",
        "\n",
        "    # Format the result\n",
        "    formatted_result = {\n",
        "        \"text\": text,\n",
        "        \"sentiment\": label_map.get(result[0][\"label\"], result[0][\"label\"]),\n",
        "        \"confidence\": round(result[0][\"score\"], 4),\n",
        "        \"inference_time\": f\"{inference_time*1000:.2f}ms\"\n",
        "    }\n",
        "\n",
        "    return formatted_result\n",
        "\n",
        "# Interactive demo\n",
        "def interactive_demo():\n",
        "    \"\"\"Run an interactive demo for sentiment analysis\"\"\"\n",
        "    pipeline = load_sentiment_pipeline()\n",
        "\n",
        "    # Example texts\n",
        "    examples = [\n",
        "        \"This movie was fantastic! The acting was superb and the plot kept me engaged throughout.\",\n",
        "        \"What a waste of time. The characters were poorly developed and the story made no sense.\",\n",
        "        \"I had mixed feelings about this film. Some parts were good, but others dragged on too long.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n===== Example Predictions =====\")\n",
        "    for example in examples:\n",
        "        result = predict(example, pipeline)\n",
        "        print(f\"\\nText: {result['text']}\")\n",
        "        print(f\"Sentiment: {result['sentiment']}\")\n",
        "        print(f\"Confidence: {result['confidence']}\")\n",
        "        print(f\"Inference time: {result['inference_time']}\")\n",
        "\n",
        "    # Custom input\n",
        "    from IPython.display import HTML, display\n",
        "    from google.colab import output\n",
        "\n",
        "    def analyze_text(text_input):\n",
        "        if text_input.strip():\n",
        "            result = predict(text_input, pipeline)\n",
        "            print(f\"\\nSentiment: {result['sentiment']}\")\n",
        "            print(f\"Confidence: {result['confidence']}\")\n",
        "            print(f\"Inference time: {result['inference_time']}\")\n",
        "\n",
        "    output.register_callback('analyze', analyze_text)\n",
        "\n",
        "    print(\"\\n===== Enter your own text to analyze =====\")\n",
        "    display(HTML('''\n",
        "    <input id=\"text_input\" style=\"width: 100%; padding: 10px; margin: 10px 0;\" placeholder=\"Enter text to analyze...\">\n",
        "    <button id=\"analyze_button\" style=\"padding: 10px 20px; background-color: #4CAF50; color: white; border: none; cursor: pointer;\">\n",
        "      Analyze Sentiment\n",
        "    </button>\n",
        "\n",
        "    <script>\n",
        "      document.getElementById('analyze_button').addEventListener('click', function() {\n",
        "        var text = document.getElementById('text_input').value;\n",
        "        google.colab.kernel.invokeFunction('analyze', [text], {});\n",
        "      });\n",
        "\n",
        "      document.getElementById('text_input').addEventListener('keypress', function(e) {\n",
        "        if (e.key === 'Enter') {\n",
        "          var text = document.getElementById('text_input').value;\n",
        "          google.colab.kernel.invokeFunction('analyze', [text], {});\n",
        "        }\n",
        "      });\n",
        "    </script>\n",
        "    '''))\n",
        "\n",
        "# Run interactive demo\n",
        "interactive_demo()"
      ],
      "metadata": {
        "id": "Jx8qFpmttcTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Complete Pipeline { display-mode: \"form\" }\n",
        "\n",
        "def run_complete_pipeline():\n",
        "    \"\"\"Run the complete pipeline from data preparation to inference\"\"\"\n",
        "    print(\"\\n===== Step 1: Data Preparation =====\")\n",
        "    global train_dataset, test_dataset\n",
        "    train_dataset, test_dataset = prepare_dataset(config[\"data\"][\"sample_size\"])\n",
        "\n",
        "    print(\"\\n===== Step 2: Model Training =====\")\n",
        "    trainer = train_model()\n",
        "\n",
        "    print(\"\\n===== Step 3: Model Evaluation =====\")\n",
        "    metrics = evaluate_model(trainer)\n",
        "\n",
        "    print(\"\\n===== Step 4: Error Analysis =====\")\n",
        "    analysis_results = error_analysis(trainer)\n",
        "\n",
        "    print(\"\\n===== Step 5: Interactive Demo =====\")\n",
        "    interactive_demo()\n",
        "\n",
        "    return {\n",
        "        \"trainer\": trainer,\n",
        "        \"metrics\": metrics,\n",
        "        \"analysis\": analysis_results\n",
        "    }\n",
        "\n",
        "# Uncomment to run the complete pipeline\n",
        "# results = run_complete_pipeline()"
      ],
      "metadata": {
        "id": "zlijMdRCt6MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save to Google Drive { display-mode: \"form\" }\n",
        "\n",
        "def save_project_to_drive():\n",
        "    \"\"\"Save the entire project to Google Drive\"\"\"\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    DRIVE_DIR = '/content/drive/MyDrive/imdb-sentiment'\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving project to Google Drive at {DRIVE_DIR}...\")\n",
        "\n",
        "    # Copy model\n",
        "    if os.path.exists(config[\"paths\"][\"best_model_dir\"]):\n",
        "        !cp -r {config[\"paths\"][\"best_model_dir\"]} {DRIVE_DIR}/\n",
        "\n",
        "    # Copy visualizations\n",
        "    if os.path.exists(config[\"paths\"][\"visualizations_dir\"]):\n",
        "        !mkdir -p {DRIVE_DIR}/visualizations\n",
        "        !cp {config[\"paths\"][\"visualizations_dir\"]}/* {DRIVE_DIR}/visualizations/\n",
        "\n",
        "    # Copy config and results\n",
        "    !cp {PROJECT_DIR}/config.json {DRIVE_DIR}/\n",
        "    if os.path.exists(os.path.join(config[\"paths\"][\"model_dir\"], \"evaluation_results.json\")):\n",
        "        !cp {os.path.join(config[\"paths\"][\"model_dir\"], \"evaluation_results.json\")} {DRIVE_DIR}/\n",
        "\n",
        "    print(\"Project saved to Google Drive!\")\n",
        "\n",
        "# Save to Google Drive\n",
        "save_project_to_drive()"
      ],
      "metadata": {
        "id": "C4vr1zDPt9Gj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}